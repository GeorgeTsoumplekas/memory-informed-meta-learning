{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from utils import load_model, get_mask\n",
    "import sys\n",
    "import os\n",
    "from argparse import Namespace\n",
    "\n",
    "from dataset.dataset import *\n",
    "from dataset.utils import get_dataloader\n",
    "from evaluation.utils import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import scienceplots\n",
    "import seaborn as sns\n",
    "\n",
    "plt.style.use(\"science\")\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(\"Dark2\")\n",
    "\n",
    "plt.rcParams[\"text.latex.preamble\"] = (\n",
    "    \"\\\\usepackage{lmodern} \\\\usepackage{times} \\\\usepackage{amssymb}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams[\"text.usetex\"] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the models\n",
    "save_dirs = {\n",
    "    \"NP\": \"../saves/INPs_sinusoids/np_0\",\n",
    "    \"INP\": \"../saves/INPs_sinusoids/inp_abc2_0\",\n",
    "}\n",
    "\n",
    "models = list(save_dirs.keys())\n",
    "model_dict = {}\n",
    "config_dict = {}\n",
    "\n",
    "for model_name, save_dir in save_dirs.items():\n",
    "    model_dict[model_name], config_dict[model_name] = load_model(\n",
    "        save_dir, load_it=\"best\"\n",
    "    )\n",
    "    model_dict[model_name].eval()\n",
    "\n",
    "model_names = list(model_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the dataloaders\n",
    "config = Namespace(\n",
    "    min_num_context=0,\n",
    "    max_num_context=100,\n",
    "    num_targets=100,\n",
    "    noise=0.2,\n",
    "    batch_size=32,\n",
    "    x_sampler=\"uniform\",\n",
    "    test_num_z_samples=32,\n",
    "    dataset=\"set-trending-sinusoids\",\n",
    "    device=\"cuda:0\",\n",
    ")\n",
    "\n",
    "dataset = SetKnowledgeTrendingSinusoids(\n",
    "    root=\"../data/trending-sinusoids\", split=\"test\", knowledge_type=\"full\"\n",
    ")\n",
    "data_loader = get_dataloader(dataset, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot sample predictions\n",
    "\n",
    "for batch in data_loader:\n",
    "    (x_context, y_context), (x_target, y_target), full_knowledge, extras = batch\n",
    "    x_context = x_context.to(config.device)\n",
    "    y_context = y_context.to(config.device)\n",
    "    x_target = x_target.to(config.device)\n",
    "    y_target = y_target.to(config.device)\n",
    "\n",
    "\n",
    "num_context_ls = [0, 1, 3, 5]\n",
    "sample_idx = np.random.choice(list(range(x_target.shape[-2])), max(num_context_ls))\n",
    "batch_idx = 0\n",
    "\n",
    "fig, axs = plt.subplots(\n",
    "    len(num_context_ls), 4, figsize=(10, 6), sharex=True, sharey=True\n",
    ")\n",
    "\n",
    "colors = {\"raw\": \"grey\", \"a\": \"C2\", \"b\": \"C2\", \"c\": \"C4\"}\n",
    "\n",
    "for j, knowledge_type in enumerate([\"raw\", \"a\", \"b\", \"c\"]):\n",
    "    if knowledge_type == \"raw\":\n",
    "        knowledge = None\n",
    "    else:\n",
    "        mask = get_mask(knowledge_type)\n",
    "        knowledge = full_knowledge * mask\n",
    "\n",
    "    for i, num_context in enumerate(num_context_ls):\n",
    "        x_context = x_target[:, sample_idx[:num_context], :]\n",
    "        y_context = y_target[:, sample_idx[:num_context], :]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            INP_outupts = model_dict[\"INP\"](\n",
    "                x_context, y_context, x_target, y_target=y_target, knowledge=knowledge\n",
    "            )\n",
    "\n",
    "        plot_predictions(\n",
    "            axs[i][j],\n",
    "            batch_idx,\n",
    "            INP_outupts,\n",
    "            x_context,\n",
    "            y_context,\n",
    "            x_target,\n",
    "            extras,\n",
    "            color=colors[knowledge_type],\n",
    "            plot_true=True,\n",
    "        )\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the models on different knowledge types\n",
    "eval_type_ls = [\"raw\", \"informed\", \"a\", \"b\", \"c\", \"ab\", \"bc\", \"ac\"]\n",
    "\n",
    "summary_df, losses, outputs_dict = get_summary_df(\n",
    "    model_dict, config_dict, data_loader, eval_type_ls, model_names\n",
    ")\n",
    "\n",
    "summary_df[\"print_value\"] = summary_df[\"mean\"].apply(\n",
    "    lambda x: f\"{x:.1f}\"\n",
    ")  # + ' \\scriptsize{(' + summary_df['std'].apply(lambda x: f'{x:.1f}') + ')}'\n",
    "print_df = (\n",
    "    summary_df.dropna(subset=[\"mean\"])\n",
    "    .pivot(\n",
    "        columns=\"num_context\", index=[\"model_name\", \"eval_type\"], values=[\"print_value\"]\n",
    "    )\n",
    "    .T.round(2)\n",
    ")\n",
    "\n",
    "print_df.droplevel(0, axis=0).dropna(axis=1, how=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df = (\n",
    "    summary_df[\n",
    "        (summary_df.model_name.isin([\"INP\", \"NP\"]))\n",
    "        & (summary_df.eval_type.isin([\"raw\", \"informed\"]))\n",
    "    ]\n",
    "    .copy()\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "plot_df[\"mean\"] = -plot_df[\"mean\"]\n",
    "plot_df[\"eval_type\"] = plot_df[\"eval_type\"].map(\n",
    "    {\n",
    "        \"raw\": r\"$\\mathcal{K} = \\varnothing$\",\n",
    "        \"informed\": r\"$\\mathcal{K} \\neq \\varnothing$\",\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(3, 3))\n",
    "sns.lineplot(\n",
    "    plot_df,\n",
    "    x=\"num_context\",\n",
    "    y=\"mean\",\n",
    "    hue=\"eval_type\",\n",
    "    style=\"model_name\",\n",
    "    palette=[\"C2\", \"C4\"],\n",
    "    ax=ax,\n",
    "    style_order=[\"NP\", \"INP\"],\n",
    "    markers=True,\n",
    ")\n",
    "\n",
    "ax.set_ylabel(\"Negative Log-likelihood\")\n",
    "ax.set_xlabel(\"Number of context datapoints\")\n",
    "\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "\n",
    "labels[0] = \"Task type:\"\n",
    "labels[3] = \"Model:\"\n",
    "\n",
    "plt.legend(labels=labels, handles=handles)\n",
    "plt.tight_layout()\n",
    "# plt.savefig('../figures/exp-1.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delta-AUC\n",
    "model_name = \"INP\"\n",
    "\n",
    "eval_type_ls = [\"a\", \"b\", \"c\", \"ab\", \"ac\", \"bc\"]\n",
    "num_context_ls = [0, 1, 3, 5, 10, 15]\n",
    "\n",
    "auc_summary, _ = get_auc_summary(losses, model_name, eval_type_ls, num_context_ls)\n",
    "\n",
    "auc_df = (pd.DataFrame(auc_summary).T * 100).round(1)\n",
    "auc_df.columns = [\"mean\", \"se\"]\n",
    "# plot bar chart with error bars\n",
    "fig, ax = plt.subplots(figsize=(3, 2))\n",
    "auc_df[\"mean\"].plot.bar(yerr=auc_df[\"se\"], ax=ax, capsize=3, color=\"C5\", alpha=0.85)\n",
    "ax.set_ylabel(\"$\\Delta$AUC [\\%]\")\n",
    "ax.set_xlabel(\"Format of $\\mathcal{K}$\")\n",
    "ax.set_xticklabels(\n",
    "    [\"$\\\\{%s\\\\}$\" % \", \".join(list(k)) for k in auc_df.index], rotation=0\n",
    ")\n",
    "plt.tight_layout()\n",
    "# plt.savefig('../figures/sinusoid_auc.pdf', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knowledge_type_ls = [\"raw\", \"informed\", \"a\", \"b\", \"c\"]\n",
    "num_context_ls = [0, 1]\n",
    "\n",
    "uncertainties = get_uncertainties(\n",
    "    outputs_dict, num_context_ls, knowledge_type_ls, model_name=\"INP\", n_batches=30\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_context = 1\n",
    "idx_ls = [(\"b\", 0, 0)] + [(\"b\", 3, 3)]\n",
    "\n",
    "fig, axs = plt.subplots(len(idx_ls), 3, figsize=(6.5, 1.8 * len(idx_ls)), sharex=True)\n",
    "\n",
    "model_name = \"INP\"\n",
    "uncert_type = \"epistemic\"\n",
    "\n",
    "for i, (knowledge_type, batch_idx, in_batch_idx) in enumerate(idx_ls):\n",
    "    uncert_reduction_knowledge = (\n",
    "        uncertainties[0][\"raw\"][batch_idx][uncert_type]\n",
    "        - uncertainties[0][knowledge_type][batch_idx][uncert_type]\n",
    "    )\n",
    "    uncert_reduction_data = (\n",
    "        uncertainties[0][\"raw\"][batch_idx][uncert_type]\n",
    "        - uncertainties[num_context][\"raw\"][batch_idx][uncert_type]\n",
    "    )\n",
    "\n",
    "    for sample_id in range(5):\n",
    "        eval_type = \"raw\"\n",
    "        mu = (\n",
    "            outputs_dict[model_name][eval_type][num_context][batch_idx][\"outputs\"][0]\n",
    "            .mean[:, :, :]\n",
    "            .cpu()\n",
    "        )\n",
    "        sigma = (\n",
    "            outputs_dict[model_name][eval_type][num_context][batch_idx][\"outputs\"][0]\n",
    "            .stddev[:, :, :]\n",
    "            .cpu()\n",
    "        )\n",
    "        x_context = outputs_dict[model_name][eval_type][num_context][batch_idx][\n",
    "            \"x_context\"\n",
    "        ]\n",
    "        y_context = outputs_dict[model_name][eval_type][num_context][batch_idx][\n",
    "            \"y_context\"\n",
    "        ]\n",
    "        x_target = outputs_dict[model_name][eval_type][num_context][batch_idx][\n",
    "            \"x_target\"\n",
    "        ]\n",
    "        y_target = outputs_dict[model_name][eval_type][num_context][batch_idx][\n",
    "            \"y_target\"\n",
    "        ]\n",
    "        # axs[i][0].plot(x_target[in_batch_idx, :].flatten(), y_target[in_batch_idx, :], color='black', linestyle='--')\n",
    "        axs[i][0].plot(\n",
    "            x_target[in_batch_idx, :].flatten(),\n",
    "            mu[sample_id, in_batch_idx, :].flatten(),\n",
    "            color=\"C4\",\n",
    "        )\n",
    "        axs[i][0].fill_between(\n",
    "            x_target[in_batch_idx, :].flatten(),\n",
    "            (\n",
    "                mu[sample_id, in_batch_idx, :] - sigma[sample_id, in_batch_idx, :]\n",
    "            ).flatten(),\n",
    "            (\n",
    "                mu[sample_id, in_batch_idx, :] + sigma[sample_id, in_batch_idx, :]\n",
    "            ).flatten(),\n",
    "            alpha=0.2,\n",
    "            color=\"C4\",\n",
    "        )\n",
    "        axs[i][0].scatter(\n",
    "            x_context[in_batch_idx, :].flatten(),\n",
    "            y_context[in_batch_idx, :].flatten(),\n",
    "            color=\"black\",\n",
    "        )\n",
    "\n",
    "        eval_type = knowledge_type\n",
    "        mu = (\n",
    "            outputs_dict[model_name][eval_type][0][batch_idx][\"outputs\"][0]\n",
    "            .mean[:, :, :]\n",
    "            .cpu()\n",
    "        )\n",
    "        sigma = (\n",
    "            outputs_dict[model_name][eval_type][0][batch_idx][\"outputs\"][0]\n",
    "            .stddev[:, :, :]\n",
    "            .cpu()\n",
    "        )\n",
    "        x_context = outputs_dict[model_name][eval_type][0][batch_idx][\"x_context\"]\n",
    "        y_context = outputs_dict[model_name][eval_type][0][batch_idx][\"y_context\"]\n",
    "        x_target = outputs_dict[model_name][eval_type][0][batch_idx][\"x_target\"]\n",
    "        # axs[i][1].plot(x_target[in_batch_idx, :].flatten(), y_target[in_batch_idx, :], color='black', linestyle='--')\n",
    "        axs[i][1].scatter(\n",
    "            x_context[in_batch_idx, :].flatten(),\n",
    "            y_context[in_batch_idx, :].flatten(),\n",
    "            color=\"black\",\n",
    "        )\n",
    "        knowledge_value = outputs_dict[model_name][eval_type][0][batch_idx][\"knowledge\"]\n",
    "        knowledge_value = (knowledge_value[in_batch_idx, :] * get_mask(knowledge_type))[\n",
    "            :, 3\n",
    "        ]\n",
    "        knowledge_value = knowledge_value[knowledge_value != 0].item()\n",
    "        label = \"$\\mathcal{K}: \\{\" + f\"{knowledge_type} = {knowledge_value:.2f}\" + \"\\}$\"\n",
    "\n",
    "        axs[i][1].plot(\n",
    "            x_target[in_batch_idx, :].flatten(),\n",
    "            mu[sample_id, in_batch_idx, :].flatten(),\n",
    "            color=\"C2\",\n",
    "            label=None if sample_id > 0 else label,\n",
    "        )\n",
    "        for item in (\n",
    "            axs[i][1]\n",
    "            .legend(handlelength=0, handletextpad=0, loc=\"lower right\", frameon=True)\n",
    "            .legendHandles\n",
    "        ):\n",
    "            item.set_visible(False)\n",
    "        axs[i][1].fill_between(\n",
    "            x_target[in_batch_idx, :].flatten(),\n",
    "            (\n",
    "                mu[sample_id, in_batch_idx, :] - sigma[sample_id, in_batch_idx, :]\n",
    "            ).flatten(),\n",
    "            (\n",
    "                mu[sample_id, in_batch_idx, :] + sigma[sample_id, in_batch_idx, :]\n",
    "            ).flatten(),\n",
    "            alpha=0.2,\n",
    "            color=\"C2\",\n",
    "        )\n",
    "    axs[i][2].plot(\n",
    "        x_target[in_batch_idx, :].flatten(),\n",
    "        uncert_reduction_data[in_batch_idx, :],\n",
    "        label=\"data\",\n",
    "        color=\"C4\",\n",
    "    )\n",
    "    axs[i][2].plot(\n",
    "        x_target[in_batch_idx, :].flatten(),\n",
    "        uncert_reduction_knowledge[in_batch_idx, :],\n",
    "        label=\"knowledge\",\n",
    "        color=\"C2\",\n",
    "    )\n",
    "\n",
    "axs[0][0].set_title(\"Informed by data \\n\" + r\"$f \\sim p_\\theta(f | \\mathcal{D}_C)$\")\n",
    "axs[0][1].set_title(\"Informed by knolwedge \\n\" + r\"$f \\sim p_\\theta(f | \\mathcal{K})$\")\n",
    "axs[0][2].set_title(\"Reduction in \\n epistemic uncertainty\")\n",
    "axs[1][2].legend(loc=\"upper left\", bbox_to_anchor=(-0.05, 1.1), frameon=True)\n",
    "\n",
    "\n",
    "for i in range(3):\n",
    "    axs[0][i].set_xticks([-2, -1, 0, 1, 2])\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.savefig('../figures/uncertainty-trending-sinusoids.pdf', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text-modulation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
